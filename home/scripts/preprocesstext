#!/usr/bin/python
import numpy as np  # linear algebra
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)
import string as st
import re
import nltk
from nltk import PorterStemmer, WordNetLemmatizer
import os
from nltk.tokenize import word_tokenize
import sys
from nltk.corpus import stopwords
import argparse
from nltk.probability import FreqDist

argparser = argparse.ArgumentParser()
argparser.add_argument("-c",type=int)
args = argparser.parse_args()

nltk.download("wordnet", quiet=True)
nltk.download("omw-1.4", quiet=True)
nltk.download("punkt", quiet=True)
nltk.download("stopwords", quiet=True)
nltk.download("words", quiet=True)
text = sys.stdin.read()


def remove_punct(text):
    return "".join([ch for ch in text if ch not in st.punctuation])


def tokenize(text):
    text = re.split("\s+", text)
    return [x.lower() for x in text]


def remove_small_words(text):
    return [x for x in text if len(x) > 3]


def stemming(text):
    ps = PorterStemmer()
    return [ps.stem(word) for word in text]


def lemmatize(text):
    word_net = WordNetLemmatizer()
    return [word_net.lemmatize(word) for word in text]


text = re.sub(r"\d+", "", text)
text = re.sub("\s+", " ", text).strip()
text = text.encode("ascii", "ignore")
text = text.decode()

text = remove_punct(text)
text = word_tokenize(text)
text = [x.lower() for x in text]
text = remove_small_words(text)
stop_words = set(stopwords.words("english"))
text = [w for w in text if w not in stop_words]

words = set(nltk.corpus.words.words())

text = [w for w in text if w in words]

text = lemmatize(text)
fdist = FreqDist(text)
text = list(filter(lambda x: x[1] <= args.c, fdist.items()))
text = [i[0] for i in text]
for w in text:
    print(w)
